package edu.umass.cs.iesl.bibie

/**
 * Created by kate on 5/13/15.
 */

import java.io._
import java.net.URL

import cc.factorie.app.nlp._
import cc.factorie.optimize._
import cc.factorie.util._
import scala.io.Source
import scala.collection.mutable.ArrayBuffer

/**
 * Infrastructure for comparison of GROBID vs IESL.
 * We load documents from a file generated by GROBID's training/eval process, which contains (a) all the features and
 * feature values GROBID uses and (b) gold labels. This way, we can more or less directly compare our learning vs.
 * GROBID's learning. One thing to note however is that GROBID uses a cascade of CRF's (I'm not 100% sure how this works
 * yet). This might be a TODO to consider implementing.
 */

/**
 * Train a model using GROBID data and features.
 */
class GrobidCitationCRFTrainer {
  val evaluator = new SegmentationEvaluation[CitationLabel](LabelDomain)
  val evaluator2 = new ExactlyLikeGrobidEvaluator
  val model = new CitationCRFModel
  def process(document: Document): Document = {
    if (document.tokens.size == 0) return document
    for (sentence <- document.sentences if sentence.tokens.size > 0) {
      val vars = sentence.tokens.map(_.attr[CitationLabel]).toSeq
      val sum = CitationBIOHelper.infer(vars, model)
      sum.setToMaximize(null)
    }
    document
  }
  def train(trainDocuments: Seq[Document], testDocuments: Seq[Document], params: Hyperparams): Unit = {
    implicit val random = new scala.util.Random
    // make "tmp" directory in root project folder ; needed for evaluations
    val tmpDir = params.root + "/tmp"
    if (!(new java.io.File(tmpDir).exists)) {
      import scala.sys.process._
      assert(s"mkdir $tmpDir".! == 0, s"could not mkdir $tmpDir")
    }
    // Get the variables to be inferred (for now, just operate on a subset)
    val trainLabels: Seq[CitationLabel] = trainDocuments.flatMap(_.tokens).map(_.attr[CitationLabel]).toSeq
    val testLabels: Seq[CitationLabel] = testDocuments.flatMap(_.tokens).map(_.attr[CitationLabel]).toSeq
    (trainLabels ++ testLabels).filter(_ != null).foreach(_.setRandomly(random))
    val vars = for (td <- trainDocuments; sentence <- td.sentences if sentence.length > 1) yield sentence.tokens.map(_.attr[CitationLabel])
    val examples = vars.map(v => new LikelihoodExample(v.toSeq, model, cc.factorie.infer.InferByBPChain))
    def evaluate(): Unit = {
      val tmpFile = tmpDir + "/" + scala.util.Random.nextInt(10000) + ".tmp"
      trainDocuments.foreach(process)
      val (trainF1, _) = evaluator2.evaluate(trainDocuments, tmpFile)
      println(s"TRAIN F1 = $trainF1")
      testDocuments.foreach(process)
      val (testF1, _) = evaluator2.evaluate(testDocuments, tmpFile)
      println(s"DEV F1 = $testF1")
    }
    params.optimizer match {
      case "lbfgs" =>
        val optimizer = new LBFGS(maxIterations=1814) with L2Regularization
        val trainer = new ThreadLocalBatchTrainer(model.parameters, optimizer)
        println("training with LBFGS ...")
        trainer.trainFromExamples(examples)
      case "adagrad" =>
        val optimizer = new AdaGradRDA(delta=params.delta, rate=params.rate, l1=params.l1, l2=params.l2, numExamples=examples.length)
        println("training with AdaGradRDA ...")
        Trainer.onlineTrain(model.parameters, examples, evaluate=evaluate, useParallelTrainer=false, maxIterations=5, optimizer=optimizer)
      case _ => throw new Exception(s"invalid optimizer: ${params.optimizer}")
    }
    (trainLabels ++ testLabels).foreach(_.setRandomly(random))
    trainDocuments.foreach(process)
    testDocuments.foreach(process)
    evaluate()
    evaluator.printEvaluation(testDocuments, testDocuments, "FINAL")
  }

  def serialize(stream: OutputStream) {
    import cc.factorie.util.CubbieConversions._
    val is = new DataOutputStream(stream)
    BinarySerializer.serialize(LabelDomain, is)
    BinarySerializer.serialize(CitationFeaturesDomain.dimensionDomain, is)
    BinarySerializer.serialize(model, is)
    is.close()
  }
  def deserialize(stream: InputStream): Unit = deserialize(model, stream)

  def deserialize(theModel: CitationCRFModel, stream: InputStream) {
    import cc.factorie.util.CubbieConversions._
    val is = new DataInputStream(stream)
    BinarySerializer.deserialize(LabelDomain, is)
    LabelDomain.freeze()
    println("deserialized labeldomain")
    BinarySerializer.deserialize(CitationFeaturesDomain.dimensionDomain, is)
    CitationFeaturesDomain.freeze()
    println("deserialized featuresdomain")
    BinarySerializer.deserialize(theModel, is)
    println("deserialized model")
    is.close()
  }
}

object TrainCitationModelGrobid extends HyperparameterMain {
  URLHandlerSetup.poke()
  def initFeatures(docs: Seq[Document]): Unit = {
    docs.flatMap(_.tokens).foreach { token =>
      token.attr += new CitationFeatures(token)
      token.attr[CitationFeatures] ++= token.attr[PreFeatures].features
    }
  }
  def evaluateParameters(args: Array[String]): Double = {
    println(args.mkString(", "))
    val opts = new TrainCitationModelOptions
    opts.parse(args)
    val params = new Hyperparams(opts)
    val trainer = new GrobidCitationCRFTrainer
    val allData = LoadGrobid.fromFilename(opts.trainFile.value)
    val trainPortion = (allData.length.toDouble * opts.trainPortion.value).floor.toInt
    val trainingData = allData.take(trainPortion)
    initFeatures(trainingData)
    CitationFeaturesDomain.freeze()
    val devData = allData.drop(trainPortion)
    initFeatures(devData)
    println(s"feature domain size: ${CitationFeaturesDomain.dimensionSize}")
    println(s"training data: ${trainingData.length} docs, ${trainingData.flatMap(_.tokens).length} tokens")
    println(s"dev data: ${devData.length} docs, ${devData.flatMap(_.tokens).length} tokens")
    trainer.train(trainingData, devData, params)
    val evaluator = new ExactlyLikeGrobidEvaluator(opts.rootDir.value)
    val (f0, eval) = evaluator.evaluate(devData, opts.outputFile.value, writeFiles=opts.writeEvals.value, outputDir=opts.outputDir.value)
    println(eval)
    if (opts.saveModel.value) {
      println(s"serializing model to: ${opts.modelFile.value}")
      trainer.serialize(new FileOutputStream(opts.modelFile.value))
    }
    f0
  }
}

object OptimizeCitationModelGrobid {
  def main(args: Array[String]): Unit = {
    println(args.mkString(", "))
    val opts = new TrainCitationModelOptions
    opts.parse(args)
    opts.saveModel.setValue(false)
    opts.writeEvals.setValue(false)
    val l1 = HyperParameter(opts.l1, new LogUniformDoubleSampler(1e-6, 1))
    val l2 = HyperParameter(opts.l2, new LogUniformDoubleSampler(1e-6, 1))
    val qs = new QSubExecutor(10, "edu.umass.cs.iesl.bibie.TrainCitationModelGrobid")
    val optimizer = new HyperParameterSearcher(opts, Seq(l1, l2), qs.execute, 200, 180, 60)
    val result = optimizer.optimize()
    println("Got results: " + result.mkString(" "))
    println("Best l1: " + opts.l1.value + " best l2: " + opts.l2.value)
    println("Running best configuration...")
    opts.saveModel.setValue(true)
    opts.writeEvals.setValue(true)
    import scala.concurrent.duration._
    import scala.concurrent.Await
    Await.result(qs.execute(opts.values.flatMap(_.unParse).toArray), 1.hours)
    println("Done.")
  }
}

object TestCitationModelGrobid {
  URLHandlerSetup.poke()
  def initFeatures(docs: Seq[Document]): Unit = {
    docs.flatMap(_.tokens).foreach { token =>
      token.attr += new CitationFeatures(token)
      token.attr[CitationFeatures] ++= token.attr[PreFeatures].features
    }
  }
  def main(args: Array[String]): Unit = {
    val opts = new TrainCitationModelOptions
    opts.parse(args)
    println(s"deserializing model from: ${opts.modelFile.value}")
    val trainer = new GrobidCitationCRFTrainer
    trainer.deserialize(new URL(opts.modelFile.value).openStream())
    CitationFeaturesDomain.freeze()
    println(s"loading file: ${opts.testFile.value}")
    val data = LoadGrobid.fromFilename(opts.testFile.value)
    initFeatures(data)
    println("processing docs")
    data.foreach(trainer.process)
    println("evaluating")
    val evaluator = new ExactlyLikeGrobidEvaluator(opts.rootDir.value)
    println(evaluator.evaluate(data, opts.outputFile.value, writeFiles=opts.writeEvals.value, outputDir=opts.outputDir.value))
    println("done.")
  }
}



class GoldCitationLabel(val label: String, val token: Token)
class PreFeatures(val features: Array[String], val token: Token)

/*
M m M M M M M M M M LINESTART ALLCAP NODIGIT 1 0 0 0 0 0 0 0 0 0 0 0 NOPUNCT 1 I-<author>
. . . . . . . . . . LINEIN ALLCAP NODIGIT 1 0 0 0 0 0 0 0 0 0 0 0 DOT 1 <author>
Kitsuregawa kitsuregawa K Ki Kit Kits a wa awa gawa LINEIN INITCAP NODIGIT 0 0 0 0 0 0 0 0 0 0 0 0 NOPUNCT 1 <author>
, , , , , , , , , , LINEIN ALLCAP NODIGIT 1 0 0 0 0 0 0 0 0 0 0 0 COMMA 1 <author>
H h H H H H H H H H LINEIN ALLCAP NODIGIT 1 0 0 0 0 0 0 0 0 0 0 0 NOPUNCT 2 <author>
. . . . . . . . . . LINEIN ALLCAP NODIGIT 1 0 0 0 0 0 0 0 0 0 0 0 DOT 2 <author>
Tanaka tanaka T Ta Tan Tana a ka aka naka LINEIN INITCAP NODIGIT 0 1 0 0 0 0 0 0 0 0 0 0 NOPUNCT 2 <author>
, , , , , , , , , , LINEIN ALLCAP NODIGIT 1 0 0 0 0 0 0 0 0 0 0 0 COMMA 3 <author>
and and a an and and d nd and and LINEIN NOCAPS NODIGIT 0 0 1 0 0 0 0 0 0 0 0 0 NOPUNCT 3 <author>
T t T T T T T T T T LINEIN ALLCAP NODIGIT 1 0 0 0 0 0 0 0 0 0 0 0 NOPUNCT 3 <author>
. . . . . . . . . . LINEIN ALLCAP NODIGIT 1 0 0 0 0 0 0 0 0 0 0 0 DOT 4 <author>
Moto moto M Mo Mot Moto o to oto Moto LINEIN INITCAP NODIGIT 0 0 0 0 0 0 0 0 0 0 0 0 NOPUNCT 4 <author>
- - - - - - - - - - LINEIN ALLCAP NODIGIT 1 0 0 0 0 0 0 0 0 0 0 0 HYPHEN 4 <author>
oka oka o ok oka oka a ka oka oka LINEIN NOCAPS NODIGIT 0 1 0 0 0 0 0 0 0 0 0 0 NOPUNCT 5 <author>
 */

object LoadGrobid {
  def fromFilename(filename: String): Seq[Document] = {
    val whitespace = "\\s+".r
    val buff = new ArrayBuffer[Document]()
    var currDoc = new Document("")
    var currSent = new Sentence(currDoc)
    val lines = Source.fromFile(filename).getLines()
    var tokenCount = 0
    var docCount = 0
    while (lines.hasNext) {
      val line = lines.next()
      val parts = whitespace.split(line)
      if (parts.length > 1) {
        val label = {
          val l = parts.last.dropRight(1)
          if (l.startsWith("I-<")) {
            val ll = l.drop(3)
            "B-" + ll
          } else {
            val ll = l.drop(1)
            "I-" + ll
          }
        }
        val string = parts.head
        val features = parts.dropRight(1)
        val token = new Token(currSent, string)
        token.attr += new PreFeatures(features, token) //put in PreFeatures so we can freeze CitationFeaturesDomain after loading training / before loading dev
        //token.attr += new CitationFeatures(token)
        //token.attr[CitationFeatures] ++= features
        token.attr += new CitationLabel(if (!LabelDomain.frozen || LabelDomain.categories.contains(label)) label else "O", token)
        tokenCount += 1
      } else {
        if (currSent.length > 0) currDoc.appendString("")
        if (currDoc.tokenCount > 0) {
          buff += currDoc
          currDoc = new Document("")
          currSent = new Sentence(currDoc)
          docCount += 1
        }
      }
    }
    println(s"Loaded $docCount docs with $tokenCount tokens from file $filename.")
    buff
  }

  def fromFilenameLabeled(filename: String): Seq[Document] = {
    val whitespace = "\\s+".r
    val buff = new ArrayBuffer[Document]()
    var currDoc = new Document("")
    var currSent = new Sentence(currDoc)
    val lines = Source.fromFile(filename).getLines()
    var tokenCount = 0
    var docCount = 0

    assert(lines.nonEmpty, s"no lines loaded from $filename")

    val okayLines = new ArrayBuffer[String]()
    try {
      while (lines.hasNext) okayLines += lines.next()
    } catch {
      case e: Exception => println(e)
    }

    for (line <- okayLines) {
//      val line = lines.next()
      val parts = whitespace.split(line)
      if (parts.length > 1) {
        val guessLabel = {
          val l = parts.last.dropRight(1)
          if (l.startsWith("I-<")) {
            val ll = l.drop(3)
            "B-" + ll
          } else {
            val ll = l.drop(1)
            "I-" + ll
          }
        }
        val trueLabel = {
          val l = parts.dropRight(1).last.dropRight(1)
          if (l.startsWith("I-<")) {
            val ll = l.drop(3)
            "B-" + ll
          } else {
            val ll = l.drop(1)
            "I-" + ll
          }
        }
        val string = parts.head
        val features = parts.dropRight(1)
        val token = new Token(currSent, string)
        token.attr += new CitationFeatures(token)
        token.attr[CitationFeatures] ++= features
        token.attr += new CitationLabel(if (!LabelDomain.frozen || LabelDomain.categories.contains(guessLabel)) guessLabel else "O", token)
        token.attr += new GoldCitationLabel(if (!LabelDomain.frozen || LabelDomain.categories.contains(trueLabel)) trueLabel else "O", token)
        tokenCount += 1
      } else {
        if (currSent.length > 0) currDoc.appendString("")
        if (currDoc.tokenCount > 0) {
          buff += currDoc
          currDoc = new Document("")
          currSent = new Sentence(currDoc)
          docCount += 1
        }
      }
    }
    println(s"Loaded $docCount docs with $tokenCount tokens from file $filename.")
    buff
  }
}



